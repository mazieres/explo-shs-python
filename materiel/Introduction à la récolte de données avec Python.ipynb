{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap & Crawl, avec Python\n",
    "\n",
    "*par [Antoine Mazieres](https://www.antonomase.fr/) et [Julie Pierson](http://www.cnrs.fr/index.php/fr/personne/julie-pierson)*\n",
    "\n",
    "Pendant cet atelier nous allons explorer diverses manières d'utiliser Python pour acquérir des données depuis des sources publiques.\n",
    "\n",
    "L'idée est assez simple : si vous pouvez afficher des informations sur votre navigateur, il existe probablement la possibilité de l'acquérir à grande échelle et de manière structurée. Par exemple : vous êtes intéressé par les données disponibles sur le site [IMDb.com](https://www.imdb.com/) à propos d'un film et voudriez pouvoir acquérir ces données pour des dizaines de milliers de films et les classer correctement dans une base de données.\n",
    "\n",
    "Nous allons dans un premier temps apprendre à accéder à une page web avec python, puis à sélectionner dans celle-ci les données qui nous intéressent. Pour cela nous allons utiliser respectivement les bilbiothèques [requests](https://requests.readthedocs.io/en/master/) et [lxml](https://lxml.de/). Cela recouvre des cas simples de ce que l'on appelle le [*web scraping*](https://en.wikipedia.org/wiki/Web_scraping).\n",
    "\n",
    "Ensuite nous allons explorer des exemples de situations où il est nécessaire de découvrir et d'accéder à de nombreuses pages pour acquérir les données qui nous intéressent. Il s'agit alors de créér un [*crawler*](https://en.wikipedia.org/wiki/Web_crawler) (\"robot d'indexation\" en français)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accéder à une page web avec Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définir l'adresse de la page\n",
    "url = \"http://perdu.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigner la page à une variable\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# page possède alors de nombreuses méthodes, comme par exemple vérifier que la requête c'est bien passé (code 200) ...\n",
    "page.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... ou si la page n'existe pas (code 404)\n",
    "requests.get(\"http://perdu.com/n-existe-pas\").status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<html><head><title>Vous Etes Perdu ?</title></head><body><h1>Perdu sur l'Internet ?</h1><h2>Pas de panique, on va vous aider</h2><strong><pre>    * <----- vous &ecirc;tes ici</pre></strong></body></html>\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher le code source de la page\n",
    "page.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parcourir la page et sélectionner des éléments\n",
    "\n",
    "Le code source de la page est écrit en HTML. Vous pouvez voir le code source de n'importe quelle page que vous visitez en faisant un `clic droit` > `afficher le code source` dans votre navigateur. C'est le fait que ce language soit structuré en balises (`<balise>`) et organise le contenu en arbre qui va nous permettre de récupérer ce qui nous intéresse. Formatés correctement, les balises font apparaitre la structure en arbre du code source :\n",
    "\n",
    "```\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Vous Etes Perdu ?</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Perdu sur l'Internet ?</h1>\n",
    "    <h2>Pas de panique, on va vous aider</h2>\n",
    "    <strong>\n",
    "      <pre>    * </pre>\n",
    "    </strong>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Pour ce déplacer dans cet arbre, on utilise un language de requête spécifique, appelé [`XPATH`](https://fr.wikipedia.org/wiki/XPath), qui permet de spécifier le lieu qui nous intéresse dans l'arborescence de la page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la bibliothèque qui nous permettra d'analyser le code HTML de la page\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le contenu de la page dans une variable\n",
    "tree = html.fromstring(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vous Etes Perdu ?']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On crée une requête xpath indiquant le chemin vers la balise <title>, \n",
    "# puis précisant qu'on veut le texte qu'elle contient\n",
    "requete_xpath = '/html/head/title/text()'\n",
    "tree.xpath(requete_xpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vous Etes Perdu ?']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comme il n'y a qu'une balise <title> dans cette page,\n",
    "# on peut aussi faire une requête plus courte pour récupérer toutes les balises de ce nom,\n",
    "# n'importe où sur la page. Pour ce faire, on commence la requête avec deux signes '/'\n",
    "requete_xpath = '//title/text()'\n",
    "tree.xpath(requete_xpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Une exemple plus réaliste**\n",
    "\n",
    "Imaginons que nous voulions récupérer la liste des titres de sections d'une page wikipedia, par exemple [la page à propos du site *perdu.com*](https://fr.wikipedia.org/wiki/Perdu.com) (donc : . Il faut que l'on inspecte \"à la main\" le code source de la page. Pour ce faire on peut utiliser une fonction du navigateur : `clic droit` sur un élément de la page qui vous intéresse, puis `inspecter l'élément`. Comme le montre l'image ci-dessous, vous accès ainsi directement à la partie du code de la page qui vous intéresse.\n",
    "\n",
    "![](https://www.antonomase.fr/img/wikipedia_inspect_code.png)\n",
    "\n",
    "L'enjeu est maintenant de trouver un motif, ou une règle, qui permettra de construire une requête XPATH permettant d'identifier tous les titres de sections. Ici on voit que la balise `<span>` dans laquelle est contenu le titre de la section (\"Description\") possède un attribut de type `class` avec la valeur `mw-headline`. Cela laisse pensé que cet élément pourrait être suffisant pour identifier les sections de la page. Pour formuler cette requête en XPATH, on utilisera les éléments suivants :\n",
    "\n",
    "- `//` : n'importe où sur la page...\n",
    "- `span` : ... séléctionner les balises \"span\"...\n",
    "- `[@class=\"mw-headline\"]` : ... qui possèdent la classe \"mw-headline\"...\n",
    "- `/text()` : ... et extraire leur contenu.\n",
    "\n",
    "`'//span[@class=\"mw-headline\"]/text()'`\n",
    "\n",
    "Il n'y a plus qu'à tester !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répétons les étapes précédentes jusqu'à obtenir l'arborescence de la page\n",
    "url = \"https://fr.wikipedia.org/wiki/Perdu.com\"\n",
    "page = requests.get(url)\n",
    "tree = html.fromstring(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Description',\n",
       " 'Nombre de visiteurs et présence sur le web',\n",
       " 'Notes et références',\n",
       " 'Voir aussi',\n",
       " 'Liens externes']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Executons notre requête XPATH\n",
    "tree.xpath('//span[@class=\"mw-headline\"]/text()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ça marche !\n",
    "\n",
    "le language de requête XPATH possède tout un tas de fonctions qui permettent de définir des motifs complexes pour identifier les éléments d'une page. Ce tutorial vous permettra d'en explorer les principales : https://www.w3schools.com/xml/xpath_intro.asp\n",
    "\n",
    "**EXERCICES**\n",
    "\n",
    "**1.** Récupérez tous les liens sur la page qui pointent vers des pages Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code de votre réponse\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Créez un *crawler* qui aille faire la même procédure (récupérer les liens internes) sur cette liste de liens, et répétez à nouveau cette procédure sur cette nouvelle liste de liens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code de votre réponse\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Créez un graph avec [NetworkX](https://networkx.github.io/) (cf. atelier précédent) du réseau des liens entre toutes les pages que vous avez visitées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code de votre réponse\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Pensez à un scénario que vous aimeriez explorer, soit pour récupérer des données, soit pour étudier la structure des liens entre les pages. Essayer de la réaliser ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code de votre réponse\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
