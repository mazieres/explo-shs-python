{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap & Crawl, avec Python\n",
    "\n",
    "*par [Antoine Mazieres](https://www.antonomase.fr/) et [Julie Pierson](http://www.cnrs.fr/index.php/fr/personne/julie-pierson)*\n",
    "\n",
    "Pendant cet atelier nous allons explorer diverses manières d'utiliser Python pour acquérir des données depuis des sources publiques.\n",
    "\n",
    "L'idée est assez simple : si vous pouvez afficher des informations sur votre navigateur, il existe probablement la possibilité de l'acquérir à grande échelle et de manière structurée. Par exemple : vous êtes intéressé par les données d'un film disponibles sur le site [IMDb.com](https://www.imdb.com/) et voudriez pouvoir acquérir ces données pour des dizaines de milliers de films et les classer correctement dans une base de données.\n",
    "\n",
    "Nous allons dans un premier temps apprendre à accéder à une page web avec python, puis à sélectionner dans celle-ci les données qui nous intéressent. Pour cela nous allons utiliser respectivement les bibliothèques [requests](https://requests.readthedocs.io/en/master/) et [lxml](https://lxml.de/). Cela recouvre des cas simples de ce que l'on appelle le [*web scraping*](https://en.wikipedia.org/wiki/Web_scraping).\n",
    "\n",
    "Ensuite nous allons explorer des exemples de situations où il est nécessaire de découvrir et d'accéder à de nombreuses pages pour acquérir les données qui nous intéressent. Il s'agit alors de créer un [*crawler*](https://en.wikipedia.org/wiki/Web_crawler) (\"robot d'indexation\" en français)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accéder à une page web avec Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définir l'adresse de la page\n",
    "url = \"http://perdu.com/\" # <- elle existe vraiment !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigner la page à une variable\n",
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la variable \"page\" possède alors de nombreuses méthodes, \n",
    "# comme par exemple vérifier que la requête s'est bien passée (code 200) ...\n",
    "page.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... ou si la page n'existe pas (code 404)\n",
    "requests.get(\"http://perdu.com/n-existe-pas\").status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le code source de la page\n",
    "page.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parcourir la page et sélectionner des éléments\n",
    "\n",
    "Le code source de la page est écrit en HTML. Vous pouvez voir le code source de n'importe quelle page que vous visitez en faisant un `clic droit` > `afficher le code source` dans votre navigateur. C'est le fait que ce langage soit structuré en balises (`<balise>`) et organise le contenu en arbre qui va nous permettre de récupérer ce qui nous intéresse. Formatés correctement, les balises font apparaître la structure en arbre du code source :\n",
    "\n",
    "```\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Vous Etes Perdu ?</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Perdu sur l'Internet ?</h1>\n",
    "    <h2>Pas de panique, on va vous aider</h2>\n",
    "    <strong>\n",
    "      <pre>    * </pre>\n",
    "    </strong>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Pour se déplacer dans cet arbre, on utilise un langage de requête spécifique, appelé [`XPATH`](https://fr.wikipedia.org/wiki/XPath), qui permet de spécifier le lieu qui nous intéresse dans l'arborescence de la page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la bibliothèque qui nous permettra d'analyser le code HTML de la page\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le contenu de la page dans une variable\n",
    "tree = html.fromstring(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée une requête xpath indiquant le chemin vers la balise <title>, \n",
    "# puis précisant qu'on veut le texte qu'elle contient\n",
    "requete_xpath = '/html/head/title/text()'\n",
    "tree.xpath(requete_xpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comme il n'y a qu'une balise <title> dans cette page,\n",
    "# on peut aussi faire une requête plus courte pour récupérer toutes les balises de ce nom,\n",
    "# n'importe où sur la page. Pour ce faire, on commence la requête avec deux signes '/'\n",
    "requete_xpath = '//title/text()'\n",
    "tree.xpath(requete_xpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Une exemple plus réaliste**\n",
    "\n",
    "Imaginons que nous voulions récupérer la liste des titres de sections d'une page Wikipedia, par exemple [la page à propos du site *perdu.com*](https://fr.wikipedia.org/wiki/Perdu.com). Il faut que l'on inspecte \"à la main\" le code source de la page. Pour ce faire on peut utiliser une fonction du navigateur : `clic droit` sur un élément de la page qui vous intéresse, puis `inspecter` (Chrome) / `examiner` (Firefox) `l'élément`. Comme le montre l'image ci-dessous, vous avez ainsi accès directement à la partie du code de la page qui vous intéresse.\n",
    "\n",
    "![](https://www.antonomase.fr/img/wikipedia_inspect_code.png)\n",
    "\n",
    "L'enjeu est maintenant de trouver un motif, ou une règle, qui permettra de construire une requête XPATH permettant d'identifier tous les titres de sections. Ici on voit que la balise `<span>` dans laquelle est contenu le titre de la section (\"Description\") possède un attribut de type `class` avec la valeur `mw-headline`. Cela laisse penser que cet élément pourrait être suffisant pour identifier les sections de la page. Pour formuler cette requête en XPATH, on utilisera les éléments suivants :\n",
    "\n",
    "- `//` : n'importe où sur la page...\n",
    "- `span` : ... séléctionner les balises \"span\"...\n",
    "- `[@class=\"mw-headline\"]` : ... qui possèdent la classe \"mw-headline\"...\n",
    "- `/text()` : ... et extraire leur contenu.\n",
    "\n",
    "`'//span[@class=\"mw-headline\"]/text()'`\n",
    "\n",
    "Il n'y a plus qu'à tester !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répétons les étapes précédentes jusqu'à obtenir l'arborescence de la page\n",
    "url = \"https://fr.wikipedia.org/wiki/Perdu.com\"\n",
    "page = requests.get(url)\n",
    "tree = html.fromstring(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executons notre requête XPATH\n",
    "tree.xpath('//span[@class=\"mw-headline\"]/text()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ça marche !\n",
    "\n",
    "Le langage de requête XPATH possède tout un tas de fonctions qui permettent de définir des motifs complexes pour identifier les éléments d'une page. Ce tutoriel vous permettra d'en explorer les principales : https://www.w3schools.com/xml/xpath_intro.asp\n",
    "\n",
    "**EXERCICES**\n",
    "\n",
    "**1.** Récupérez tous les liens sur la page qui pointent vers des pages Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code de votre réponse\n",
    "def get_internal_links(url):\n",
    "    page = requests.get(url)\n",
    "    tree = html.fromstring(page.text)\n",
    "    internal_links = set(tree.xpath(\"//a[starts-with(@href, '/wiki/')]/@href\"))\n",
    "    return list(internal_links)\n",
    "\n",
    "internal_links = get_internal_links(\"https://fr.wikipedia.org/wiki/Perdu.com\")\n",
    "internal_links[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Créez un *crawler* qui aille faire la même procédure (récupérer les liens internes) sur cette liste de liens et sauvegardez ces données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code de votre réponse\n",
    "links = {}\n",
    "for url in internal_links:\n",
    "    new_internal_links = get_internal_links(\"https://fr.wikipedia.org\" + url)\n",
    "    links[url] = new_internal_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Obtenir les dates des films du top 100 IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exercice pour mettre en pratique ce que nous avons vu, en regardant les dates des films du top 100 IMDb. L'intérêt de se limiter à 100 films est que les requêtes s'exécutent rapidement ! L'idée est ici d'explorer rapidement une variable (la date de sortie des films)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour obtenir l'arborescence de la page correspondant au top 100\n",
    "url = \"https://www.imdb.com/search/title/?groups=top_100&count=100\"\n",
    "page = requests.get(url)\n",
    "tree = html.fromstring(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour stocker la liste des titres\n",
    "liste_titres = tree.xpath('//h3/a/text()')\n",
    "# affichage des 10 premiers\n",
    "liste_titres[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour stocker la liste des années correspondantes\n",
    "liste_annees = tree.xpath('//h3/span[@class=\"lister-item-year text-muted unbold\"]/text()')\n",
    "# affichage des 10 premiers\n",
    "liste_annees[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour ne garder que l'année, sans les parenthèses, en convertissant en nombre entier\n",
    "# (on peut aussi utiliser une boucle for)\n",
    "liste_annees = [int(s[-5:-1]) for s in liste_annees]\n",
    "liste_annees[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalement on a autant de titres que d'années\n",
    "print ('nb titres : ' + str(len(liste_titres)))\n",
    "print ('nb années : ' + str(len(liste_annees)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour facilement visualiser les données, on en fait un dataframe avec 2 colonnes titre et annee\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(liste_titres, columns=['titre'])\n",
    "df['annee'] = liste_annees\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et si on veut représenter ces données pour en avoir un rapide aperçu :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# il faut convertir l'année en date\n",
    "df['annee'] = pd.to_datetime(df['annee'], format='%Y')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# et on peut faire un graphique rapide du nombre de films par année\n",
    "df2 = df['annee'].value_counts()\n",
    "df2.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Pensez à un scénario que vous aimeriez explorer, soit pour récupérer des données, soit pour étudier la structure des liens entre les pages. Essayer de la réaliser ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le code de votre réponse\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
